{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e60469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HuggingFace cache set to: c:\\Users\\Birhanu Matebe\\Downloads\\KAIM\\RAG-Powered\\RAG-Powered-Chatbot\\rag-compliant-chatbot\\models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "sys.path.append(project_root)\n",
    "from src.vectorStore import VectorStoreManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a917cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"../data/processed/complaints_processed_for_rag.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc04cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Embedding Model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stratified sample of 12000 records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Birhanu Matebe\\Downloads\\KAIM\\RAG-Powered\\RAG-Powered-Chatbot\\rag-compliant-chatbot\\src\\vectorStore.py:43: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('Product', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking 11999 narratives...\n",
      "Generating embeddings for 20400 chunks (this may take a few minutes)...\n",
      "✅ Vector store saved to ../vector_store/complaints_index\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the manager (This sets the HF cache and loads the model)\n",
    "vector_manager = VectorStoreManager(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Create the Stratified Sample (10,000 - 15,000 as per Task 2)\n",
    "# df_cleaned is the result from your Task 1 Processor\n",
    "sampled_df = vector_manager.prepare_stratified_sample(df, target_size=12000)\n",
    "\n",
    "# 3. Build the Index (Chunking -> Embedding -> Indexing)\n",
    "vector_db = vector_manager.build_and_save_index(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936fb9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Product: Credit card\n",
      "Complaint ID: 12483979\n",
      "Text Chunk: credit card identify steal unauthorized transaction numerous place fast amount time dispute transaction discover come back say fast enough report false call freeze credit card say pay item never purch...\n",
      "\n",
      "--- Result 2 ---\n",
      "Product: Credit card\n",
      "Complaint ID: 7826217\n",
      "Text Chunk: fraudulent activity discover credit card...\n"
     ]
    }
   ],
   "source": [
    "# Quick Test Search\n",
    "test_query = \"problems with unauthorized transactions on my credit card\"\n",
    "search_results = vector_db.similarity_search(test_query, k=2)\n",
    "\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Product: {doc.metadata['product']}\")\n",
    "    print(f\"Complaint ID: {doc.metadata['complaint_id']}\")\n",
    "    print(f\"Text Chunk: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec4125",
   "metadata": {},
   "source": [
    "## Text Chunking, Embedding, and Vector Store Indexing\n",
    "\n",
    "Sampling Strategy: \"A stratified sample of 12,000 records was successfully extracted, ensuring proportional representation across all product categories.\"\n",
    "\n",
    "Chunking Approach: \"We implemented a RecursiveCharacterTextSplitter with a chunk size of 600 and an overlap of 60. This preserves local context while optimizing for the 384-dimension limit of the embedding model.\"\n",
    "\n",
    "Embedding & Indexing: \"The all-MiniLM-L6-v2 model was used to generate embeddings. The resulting vectors and their associated metadata (ID, Product, Issue) were persisted locally using a FAISS index, as evidenced by the generated index.faiss and index.pkl files.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
